# -*- coding: utf-8 -*-
"""INSAID.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1qgg2AgQAsyCAKQy3FDWwctOCoOotRBT_

# Installing Required Libraries
"""

!pip install pyspark

!pip install findspark

"""# Importing Pyspark Library"""

from pyspark.sql import SparkSession
import findspark

findspark.init()
print('The working directory of pyspark in google colab is',findspark.find())

"""# Creating Spark Session"""

spark=SparkSession.builder.appName("DataFrame").getOrCreate()
spark

"""# Reading The Uploded CSV File With Pyspark"""

df_pyspark=spark.read.csv('Fraud.csv',header=True,inferSchema=True)
df_pyspark

df_pyspark.show()

"""# Schema Of The Given Dataset """

df_pyspark.printSchema()

"""# Finding Count of Null, None, NaN of All DataFrame Columns"""

from pyspark.sql.functions import col,isnan, when, count
df_pyspark.select([count(when(isnan(c) | col(c).isNull(), c)).alias(c) for c in df_pyspark.columns]).show()

"""# Creating Vector"""

required_features=['step','amount','oldbalanceOrg','newbalanceOrig','oldbalanceDest','newbalanceDest','isFraud','isFlaggedFraud']

from pyspark.ml.feature import VectorAssembler

assembler = VectorAssembler(inputCols=required_features, outputCol='features')

transformed_data = assembler.transform(df_pyspark)
transformed_data.show()

"""# Spliting The Data Into Training And Testing Data"""

# Split the data
training_data, test_data = transformed_data.randomSplit([0.8,0.2], seed =2020)
training_data.show()
print("Training Dataset Count: " + str(training_data.count()))
print('***********************************************')
test_data.show()
print("Test Dataset Count: " + str(test_data.count()))

"""# Using Random Forest Classifier"""

from pyspark.ml.classification import RandomForestClassifier

rf = RandomForestClassifier(labelCol='isFraud', 
                            featuresCol='features',
                            maxDepth=5
                            )
model = rf.fit(training_data)
rf_predictions = model.transform(test_data)

"""# Using Evaluator To Predict The Accuracy Of The Random Forest Model"""

from pyspark.ml.evaluation import MulticlassClassificationEvaluator

multi_evaluator = MulticlassClassificationEvaluator(labelCol = 'isFraud', metricName = 'accuracy')
print('Random Forest Classifier Accuracy:', multi_evaluator.evaluate(rf_predictions))

"""# Using Decision Tree Classifier"""

from pyspark.ml.classification import DecisionTreeClassifier

rf = DecisionTreeClassifier(labelCol='isFraud', 
                            featuresCol='features',
                            maxDepth=5)
model = rf.fit(training_data)
rf_predictions = model.transform(test_data)

"""# Using Evaluator To Predict The Accuracy Of The Decision Tree Model"""

from pyspark.ml.evaluation import MulticlassClassificationEvaluator

multi_evaluator = MulticlassClassificationEvaluator(labelCol = 'isFraud', metricName = 'accuracy')
print('Decision Tree Classifier Accuracy:', multi_evaluator.evaluate(rf_predictions))

import pandas as pd 
import matplotlib.pyplot as plt

df = pd.read_csv("Fraud1.csv")
df.head()

df.boxplot(column =['amount'], grid = False)

df.boxplot(column =['oldbalanceOrg'], grid = False)

df.boxplot(column =['newbalanceOrig'], grid = False)

df.boxplot(column =['oldbalanceDest'], grid = False)

df.boxplot(column =['newbalanceDest'], grid = False)

df.boxplot(column =['isFraud'], grid = False)

import numpy as np
from statsmodels.stats.outliers_influence import variance_inflation_factor

df = pd.read_csv('Fraud.csv')
df = df[~df.isin([np.nan, np.inf, -np.inf]).any(1)]
X_variables = df[['amount','oldbalanceOrg','newbalanceOrig','oldbalanceDest','newbalanceDest']]

vif_data = pd.DataFrame()
vif_data["feature"] = X_variables.columns
vif_data["VIF"] = [variance_inflation_factor(X_variables.values, i) for i in range(len(X_variables.columns))]

print(vif_data['VIF'])

